import random
import copy
import json
from .titans_pathfinder import ExecutionTitan, JanusTitan, PerformanceTitan, _engineer_fingerprint_from_telemetry
from rich import print

# [DEFINITIVE - V22.0 "OPERATION OMEGA" - THE FINAL FOUNDRY]
# This version integrates the definitive "Omega Fitness Function."
# It moves beyond simple binary scoring to a scientifically rigorous model that
# rewards genomes based on the AI's statistical confidence, the decisiveness
# of the defense, and the overall efficiency of the policy. This is the
# final, most intelligent version of the evolutionary engine.

class SentinelFoundry:
    def __init__(self, config):
        self.config = config
        self.population_size = config.get("population_size", 20)
        self.generations = config.get("generations", 50)
        self.elitism_count = config.get("elitism_count", 2)
        self.mutation_rate = config.get("mutation_rate", 0.9)
        self.crossover_rate = config.get("crossover_rate", 0.7)
        self.mutation_strength = config.get("mutation_strength", 0.2)

        self.METRICS = ['avg_cpu_percent', 'max_cpu_percent', 'avg_memory_rss_bytes', 'max_memory_rss_bytes', 'avg_io_write_bytes', 'max_io_write_bytes', 'avg_io_read_bytes', 'max_io_read_bytes', 'avg_num_threads', 'max_num_threads', 'profile_id']
        self.RULE_OPERATORS = ['GT', 'LT', 'EQ', 'NEQ']
        self.LOGIC_OPERATORS = ['AND', 'OR', 'XOR', 'NAND', 'NOR']

        self.population = []
        self.execution_titan = ExecutionTitan()
        self.janus_titan = JanusTitan()
        self.performance_titan = PerformanceTitan()
        self.epoch = 0
        self.normal_profile_id = -1 

        self.benign_payloads = [b'{"name": "COSMOS"}', b'{"version": 1.0}']
        self.attack_payloads = [b'A' * 512, b'{"key": "%s%s"}']

    def calibrate(self):
        print("Calibrating... Determining 'Normal' behavioral profile...")
        permissive_genome = {} 
        run_result = self.execution_titan.instrumented_run(self.benign_payloads[0], genome=permissive_genome)
        profile_result = self.performance_titan.analyze(run_result.get('raw_telemetry', []))
        self.normal_profile_id = profile_result['profile']
        if self.normal_profile_id == -1:
            raise RuntimeError("FATAL: Calibration failed! Digital Twin could not identify a normal profile.")
        print(f"[bold green]Calibration Complete. 'Normal' behavior is Profile ID: {self.normal_profile_id}[/bold green]")

    def _create_random_rule(self) -> dict:
        metric = random.choice(self.METRICS)
        if 'cpu' in metric: value = random.uniform(5.0, 95.0)
        elif 'memory' in metric: value = random.uniform(1_000_000, 100_000_000)
        elif 'io' in metric: value = random.uniform(1_024, 500_000)
        elif 'threads' in metric: value = random.uniform(1, 16)
        elif 'profile_id' in metric: value = random.randint(0, 4)
        else: value = 0
        return {"type": "rule", "metric": metric, "operator": random.choice(self.RULE_OPERATORS), "value": round(value, 2)}

    def _create_random_logic_node(self, max_depth=2, current_depth=0) -> dict:
        if current_depth >= max_depth or random.random() < 0.4:
            return self._create_random_rule()
        else:
            children = [self._create_random_logic_node(max_depth, current_depth + 1), self._create_random_logic_node(max_depth, current_depth + 1)]
            return {"operator": random.choice(self.LOGIC_OPERATORS), "children": children}

    def _create_random_state(self, state_name: str) -> dict:
        return {"comment": f"Autogenerated state: {state_name}", "active_policy": self._create_random_logic_node(max_depth=2), "transitions": []}

    def _create_random_architect_genome(self) -> dict:
        num_states = random.randint(2, 4); state_names = [f"STATE_{i}" for i in range(num_states)]
        genome = {"initial_state": "STATE_0", "variables": {}, "states": {name: self._create_random_state(name) for name in state_names}}
        for state_name in state_names:
            num_transitions = random.randint(0, 2)
            for _ in range(num_transitions):
                target_state = random.choice(state_names)
                if target_state != state_name:
                    genome["states"][state_name]["transitions"].append({"target_state": target_state, "condition": self._create_random_logic_node(max_depth=1)})
        return genome

    def _initialize_population(self):
        for i in range(self.population_size):
            self.population.append({'genome': self._create_random_architect_genome(), 'fitness': -9999, 'breakdown': {}, 'id': i})

    def _evaluate_genome(self, individual: dict) -> dict:
        """
        [OMEGA FITNESS FUNCTION - V2.0]
        This is the definitive, scientifically rigorous fitness function.
        It uses a continuous scoring model based on the AI's statistical confidence
        and the efficiency of the defense, rather than simple binary scores.
        """
        genome = individual['genome']
        total_fitness = 0
        breakdown = {}

        # --- 1. Correctness (Scored by Confidence) ---
        normal_run_result = self.execution_titan.instrumented_run(self.benign_payloads[0], genome)
        normal_profile_analysis = self.performance_titan.analyze(normal_run_result.get('raw_telemetry', []))
        
        correctness_score = 0
        if normal_run_result['outcome'] == 'survived':
            confidence = normal_profile_analysis.get('confidence', {}).get(str(self.normal_profile_id), 0.0)
            correctness_score = 500 * (confidence ** 2)
        else:
            correctness_score = -2000.0
        
        total_fitness += correctness_score
        breakdown['Correctness'] = correctness_score

        # --- 2. Security (Scored by Decisiveness) ---
        attack_run_result = self.execution_titan.instrumented_run(self.attack_payloads[0], genome)
        
        security_score = 0
        if attack_run_result['outcome'] != 'survived':
            telemetry_len = len(attack_run_result.get('raw_telemetry', []))
            if telemetry_len > 0:
                # The faster the detection (fewer telemetry points), the higher the score.
                security_score = 1000 / (1 + telemetry_len)
            else:
                 security_score = 1000.0 # Instant detection
        else:
            security_score = -1000.0
            
        total_fitness += security_score
        breakdown['Security'] = security_score

        # --- 3. Performance & Elegance (Penalties) ---
        if correctness_score > 0:
            fingerprint = _engineer_fingerprint_from_telemetry(normal_run_result.get('raw_telemetry',[]), self.performance_titan.feature_list)
            perf_overhead = fingerprint['cpu_percent_total_mean'].iloc[0] if not fingerprint.empty else 100.0
            perf_penalty = - (perf_overhead ** 1.5)
            total_fitness += perf_penalty
            breakdown['Performance Penalty'] = perf_penalty
            
            complexity = len(json.dumps(genome)) / 100
            elegance_penalty = -complexity * 10
            total_fitness += elegance_penalty
            breakdown['Elegance Penalty'] = elegance_penalty
            
        individual.update({'fitness': total_fitness, 'breakdown': breakdown})
        return individual

    def _crossover(self, parent1: dict, parent2: dict) -> tuple:
        child1_genome, child2_genome = copy.deepcopy(parent1['genome']), copy.deepcopy(parent2['genome'])
        if len(child1_genome['states']) > 0 and len(child2_genome['states']) > 0:
            state1_key, state2_key = random.choice(list(child1_genome['states'].keys())), random.choice(list(child2_genome['states'].keys()))
            child1_genome['states'][state1_key]['active_policy'], child2_genome['states'][state2_key]['active_policy'] = \
            child2_genome['states'][state2_key]['active_policy'], child1_genome['states'][state1_key]['active_policy']
        return child1_genome, child2_genome

    def _mutate_genome(self, genome: dict):
        try:
            state_to_mutate = random.choice(list(genome['states'].values()))
            policy_tree = state_to_mutate['active_policy']
            if 'type' in policy_tree and policy_tree['type'] == 'rule':
                policy_tree['value'] *= (1.0 + random.uniform(-self.mutation_strength, self.mutation_strength))
        except (IndexError, KeyError): pass
        return genome

    def _evolve_population(self):
        self.population.sort(key=lambda x: x['fitness'], reverse=True)
        elites = [copy.deepcopy(ind) for ind in self.population[:self.elitism_count]]
        new_population = elites
        while len(new_population) < self.population_size:
            if random.random() < self.crossover_rate and len(self.population) >= 2:
                p1, p2 = random.sample(self.population, k=2)
                c1_genome, c2_genome = self._crossover(p1, p2)
                new_population.append({'genome': c1_genome, 'fitness': -9999, 'breakdown': {}, 'id': len(new_population)})
                if len(new_population) < self.population_size: new_population.append({'genome': c2_genome, 'fitness': -9999, 'breakdown': {}, 'id': len(new_population)})
            else:
                participants = random.sample(self.population, k=5)
                winner = max(participants, key=lambda x: x['fitness'])
                new_population.append(copy.deepcopy(winner))
        for i in range(self.elitism_count, len(new_population)):
            if random.random() < self.mutation_rate: new_population[i]['genome'] = self._mutate_genome(new_population[i]['genome'])
        self.population = new_population[:self.population_size]