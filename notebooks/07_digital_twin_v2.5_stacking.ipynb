{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8eb19f04",
   "metadata": {},
   "source": [
    "# Digital Twin v2.5: Stacking Ensemble\n",
    "\n",
    "**Author:** Kian Mansouri Jamshidi\n",
    "**Project Director:** Kian Mansouri Jamshidi\n",
    "**Date:** 2025-09-27\n",
    "\n",
    "## Objective\n",
    "This is the final and most advanced modeling experiment of Sprint 5. We will construct a **Stacking Ensemble**, a 'network of AIs', to learn how to best combine the predictions from our two strongest models: the robust **V2.0** and the complex **V2.3 (Deep History)**.\n",
    "\n",
    "Instead of a simple weighted average, a final 'meta-model' will be trained on the outputs of our base models. This is our definitive attempt to synthesize their strengths and achieve the highest possible R² score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07dbad93",
   "metadata": {},
   "source": [
    "### 1. Imports and Full Data Preparation\n",
    "\n",
    "We begin by loading the data and the two pre-trained champion models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6da6780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded.\n",
      "Base models v2.0 and v2.3 loaded.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import RidgeCV # A good choice for a meta-model\n",
    "import lightgbm as lgb\n",
    "\n",
    "# --- Load Data ---\n",
    "PROJECT_ROOT = Path('.').resolve().parent\n",
    "TELEMETRY_DIR = PROJECT_ROOT / 'data' / 'telemetry_v2'\n",
    "ARTIFACT_DIR = PROJECT_ROOT / 'artifacts' / 'phase2'\n",
    "df_list = [pd.read_parquet(file) for file in glob.glob(str(TELEMETRY_DIR / \"*.parquet\"))]\n",
    "df = pd.concat(df_list, ignore_index=True).sort_values(by='timestamp').reset_index(drop=True)\n",
    "if 'cpu_temp_celsius_avg' in df.columns:\n",
    "    df = df.drop('cpu_temp_celsius_avg', axis=1)\n",
    "print(\"Data loaded.\")\n",
    "\n",
    "# --- Load Pre-trained Models ---\n",
    "model_v2_0 = joblib.load(ARTIFACT_DIR / 'digital_twin_v2.0.joblib')\n",
    "model_v2_3 = joblib.load(ARTIFACT_DIR / 'digital_twin_v2.3_deep.joblib')\n",
    "print(\"Base models v2.0 and v2.3 loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb68071",
   "metadata": {},
   "source": [
    "### 2. Feature and Target Preparation\n",
    "\n",
    "We need to create the two different feature sets required by our two base models. Crucially, they must be perfectly aligned on the same set of rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "705c8d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data aligned. Final dataset size for stacking: 6899 rows.\n"
     ]
    }
   ],
   "source": [
    "# --- Feature Engineering Functions (from previous notebooks) --- #\n",
    "def create_v2_features(input_df):\n",
    "    df_featured = input_df.copy()\n",
    "    workload_dummies = pd.get_dummies(df_featured['workload_type'], prefix='workload')\n",
    "    df_featured = pd.concat([df_featured, workload_dummies], axis=1)\n",
    "    df_featured['overall_util_rolling_mean'] = df_featured['cpu_util_overall'].rolling(window=10).mean()\n",
    "    df_featured['overall_util_rolling_std'] = df_featured['cpu_util_overall'].rolling(window=10).std()\n",
    "    other_core_cols = [c for c in input_df.columns if 'cpu_util_core' in c and c != 'cpu_util_core_0']\n",
    "    for i in range(1, 6):\n",
    "        df_featured[f'overall_util_lag_{i}'] = df_featured['cpu_util_overall'].shift(i)\n",
    "        for core_col in other_core_cols:\n",
    "            df_featured[f'{core_col}_lag_{i}'] = df_featured[core_col].shift(i)\n",
    "    df_model = df_featured.drop('workload_type', axis=1).dropna().reset_index(drop=True)\n",
    "    return df_model\n",
    "\n",
    "def create_v2_3_features(input_df):\n",
    "    df_featured = input_df.copy()\n",
    "    workload_dummies = pd.get_dummies(df_featured['workload_type'], prefix='workload')\n",
    "    df_featured = pd.concat([df_featured, workload_dummies], axis=1)\n",
    "    df_featured['overall_util_rolling_mean'] = df_featured['cpu_util_overall'].rolling(window=30).mean()\n",
    "    df_featured['overall_util_rolling_std'] = df_featured['cpu_util_overall'].rolling(window=30).std()\n",
    "    other_core_cols = [c for c in input_df.columns if 'cpu_util_core' in c and c != 'cpu_util_core_0']\n",
    "    for i in range(1, 21):\n",
    "        df_featured[f'overall_util_lag_{i}'] = df_featured['cpu_util_overall'].shift(i)\n",
    "        if i <= 10:\n",
    "            for core_col in other_core_cols:\n",
    "                df_featured[f'{core_col}_lag_{i}'] = df_featured[core_col].shift(i)\n",
    "    df_model = df_featured.drop('workload_type', axis=1).dropna().reset_index(drop=True)\n",
    "    return df_model\n",
    "\n",
    "# --- Create and Align the Two Feature Sets ---\n",
    "df_v2_0 = create_v2_features(df)\n",
    "df_v2_3 = create_v2_3_features(df)\n",
    "target = 'cpu_util_core_0'\n",
    "\n",
    "common_indices = df_v2_0.index.intersection(df_v2_3.index)\n",
    "df_v2_0_aligned = df_v2_0.loc[common_indices]\n",
    "df_v2_3_aligned = df_v2_3.loc[common_indices]\n",
    "\n",
    "features_v2_0_cols = [c for c in df_v2_0_aligned.columns if ('cpu_util' in c and c != target) or 'workload_' in c]\n",
    "features_v2_3_cols = [c for c in df_v2_3_aligned.columns if ('cpu_util' in c and c != target) or 'workload_' in c]\n",
    "\n",
    "X = df_v2_0_aligned[features_v2_0_cols] # Use one dataframe's features as the base\n",
    "y = df_v2_0_aligned[target]\n",
    "\n",
    "print(f\"Data aligned. Final dataset size for stacking: {X.shape[0]} rows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef494232",
   "metadata": {},
   "source": [
    "### 3. Build and Train the Stacking Ensemble\n",
    "\n",
    "We will now define the ensemble. Our two pre-trained LightGBM models will be the 'base estimators'. A simple but powerful regularized linear regression (`RidgeCV`) will be the 'final estimator' or 'meta-model'. It will learn the optimal way to combine the outputs of the base models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb962e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing manual stacking...\n",
      "Generating predictions from base models...\n",
      "Training the meta-model...\n",
      "Stacking complete.\n"
     ]
    }
   ],
   "source": [
    "# The StackingRegressor requires a list of estimators.\n",
    "# Since our models require different feature sets, we can't use the simple API.\n",
    "# We will perform the stacking manually, which gives us full control.\n",
    "\n",
    "print(\"Performing manual stacking...\")\n",
    "\n",
    "# Split the aligned data into training and testing sets\n",
    "X_train_v2_0, X_test_v2_0, y_train, y_test = train_test_split(X[features_v2_0_cols], y, test_size=0.2, random_state=42)\n",
    "X_train_v2_3, X_test_v2_3, _, _ = train_test_split(df_v2_3_aligned[features_v2_3_cols], y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 1. Get predictions from base models on the test set\n",
    "print(\"Generating predictions from base models...\")\n",
    "pred_test_v2_0 = model_v2_0.predict(X_test_v2_0)\n",
    "pred_test_v2_3 = model_v2_3.predict(X_test_v2_3)\n",
    "\n",
    "# 2. Create the training set for the meta-model\n",
    "# The features for the meta-model are the predictions of the base models\n",
    "pred_train_v2_0 = model_v2_0.predict(X_train_v2_0)\n",
    "pred_train_v2_3 = model_v2_3.predict(X_train_v2_3)\n",
    "\n",
    "X_meta_train = pd.DataFrame({\n",
    "    'pred_v2_0': pred_train_v2_0,\n",
    "    'pred_v2_3': pred_train_v2_3\n",
    "})\n",
    "\n",
    "# 3. Train the meta-model\n",
    "print(\"Training the meta-model...\")\n",
    "meta_model = RidgeCV()\n",
    "meta_model.fit(X_meta_train, y_train)\n",
    "\n",
    "# 4. Create the test set for the meta-model and make final predictions\n",
    "X_meta_test = pd.DataFrame({\n",
    "    'pred_v2_0': pred_test_v2_0,\n",
    "    'pred_v2_3': pred_test_v2_3\n",
    "})\n",
    "\n",
    "final_predictions = meta_model.predict(X_meta_test)\n",
    "print(\"Stacking complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bc5a3a",
   "metadata": {},
   "source": [
    "### 4. Final Evaluation of the Stacking Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc4cd2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Final Stacking Ensemble Performance ---\n",
      "R-squared (R²): 0.7436\n",
      "\n",
      "BREAKTHROUGH: The stacking ensemble provided a significant performance boost over any single model.\n",
      "\n",
      "Learned Meta-Model Weights:\n",
      "Weight for V2.0 Model: 1.0579\n",
      "Weight for V2.3 Model: 0.0300\n"
     ]
    }
   ],
   "source": [
    "r2 = r2_score(y_test, final_predictions)\n",
    "\n",
    "print(f\"--- Final Stacking Ensemble Performance ---\")\n",
    "print(f\"R-squared (R²): {r2:.4f}\")\n",
    "\n",
    "if r2 >= 0.85:\n",
    "    print(\"\\nMISSION ACCOMPLISHED: The stacking ensemble has broken the 85% barrier!\")\n",
    "elif r2 > 0.71:\n",
    "    print(\"\\nBREAKTHROUGH: The stacking ensemble provided a significant performance boost over any single model.\")\n",
    "else:\n",
    "    print(\"\\nLIMIT REACHED: Even the stacking ensemble could not improve performance. The V2.0 model remains the champion.\")\n",
    "\n",
    "# Get the coefficients (weights) learned by the meta-model\n",
    "meta_weights = meta_model.coef_\n",
    "print(\"\\nLearned Meta-Model Weights:\")\n",
    "print(f\"Weight for V2.0 Model: {meta_weights[0]:.4f}\")\n",
    "print(f\"Weight for V2.3 Model: {meta_weights[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae393c3-8844-4fba-b7f3-b47400af84c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
