{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0a0a0a0",
   "metadata": {},
   "source": [
    "# Digital Twin v6.0: The Lexical Hybrid Ensemble (Definitive)\n",
    "\n",
    "**NOTE:** This is V2 of the notebook. It includes a diagnostic cell to solve any `ModuleNotFoundError` issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b1b1b1",
   "metadata": {},
   "source": [
    "### 1. Diagnostic & Path Validation\n",
    "\n",
    "**Instructions:** Run this cell first. It will validate the project paths and ensure the necessary files are visible to the Jupyter kernel. If this cell succeeds, you can proceed to run the rest of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2c2c2c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated Project Root: /home/kian/Desktop/Clean_Projects/ForgeX4-COSMOS-Omega\n",
      "Python Executable:       /home/kian/miniconda3/envs/cosmos/bin/python3.10\n",
      "--- Checking for Target File ---\n",
      "Checking in directory: /home/kian/Desktop/Clean_Projects/ForgeX4-COSMOS-Omega/cosmos/foundry\n",
      "Target file to import: /home/kian/Desktop/Clean_Projects/ForgeX4-COSMOS-Omega/cosmos/foundry/feature_extractor_lexical.py\n",
      "Target file exists:    False\n",
      "--- Contents of cosmos/foundry/ ---\n",
      "['titans.py', 'eature_extractor_lexical.py', 'foundry.py', 'titans_sentinel.py', 'mutations', 'foundry_sentinel.py', '__pycache__', 'feature_extractor.py', 'uranus_evolver.py', 'fitness.py']\n",
      "\n",
      "[SUCCESS] Paths appear correct. You may now proceed.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# --- Definitive Path Validation ---\n",
    "PROJECT_ROOT = Path('.').resolve().parent\n",
    "# Insert at the front of the path to guarantee it's checked first\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "print(f\"Calculated Project Root: {PROJECT_ROOT}\")\n",
    "print(f\"Python Executable:       {sys.executable}\")\n",
    "\n",
    "# --- Check for the existence of the critical file ---\n",
    "foundry_dir = PROJECT_ROOT / 'cosmos' / 'foundry'\n",
    "target_file = foundry_dir / 'feature_extractor_lexical.py'\n",
    "print(f\"--- Checking for Target File ---\")\n",
    "print(f\"Checking in directory: {foundry_dir}\")\n",
    "print(f\"Target file to import: {target_file}\")\n",
    "print(f\"Target file exists:    {target_file.exists()}\")\n",
    "\n",
    "print(\"--- Contents of cosmos/foundry/ ---\")\n",
    "try:\n",
    "    print(os.listdir(foundry_dir))\n",
    "    print(\"\\n[SUCCESS] Paths appear correct. You may now proceed.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"[ERROR] The 'cosmos/foundry' directory was not found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d3d3d3",
   "metadata": {},
   "source": [
    "### 2. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1b2c3d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Definitive v6.0 model artifacts will be saved to: /home/kian/Desktop/Clean_Projects/ForgeX4-COSMOS-Omega/artifacts/phase2/digital_twin_v6.0_lexical_ensemble\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import joblib\n",
    "import copy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# --- THE DEFINITIVE FIX: Import the new, robust lexical extractor ---\n",
    "from cosmos.foundry.feature_extractor_lexical import extract_lexical_features\n",
    "from cosmos.foundry.feature_extractor import create_time_series_features # We still use this for dynamic features\n",
    "\n",
    "TELEMETRY_DIR = PROJECT_ROOT / 'data' / 'telemetry_v2'\n",
    "TARGET_FILE_PATH = PROJECT_ROOT / 'data' / 'genomes' / 'cjson' / 'cJSON.c'\n",
    "ARTIFACT_DIR = PROJECT_ROOT / 'artifacts' / 'phase2'\n",
    "\n",
    "ENSEMBLE_DIR = ARTIFACT_DIR / 'digital_twin_v6.0_lexical_ensemble'\n",
    "ENSEMBLE_DIR.mkdir(exist_ok=True)\n",
    "print(f\"Definitive v6.0 model artifacts will be saved to: {ENSEMBLE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3d4e5",
   "metadata": {},
   "source": [
    "### 3. Definitive Lexical Hybrid Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3d4e5f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating lexical fingerprint for cJSON.c...\n",
      "Lexical features extracted successfully.\n",
      "Loading telemetry data from: /home/kian/Desktop/Clean_Projects/ForgeX4-COSMOS-Omega/data/telemetry_v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kian/Desktop/Clean_Projects/ForgeX4-COSMOS-Omega/cosmos/foundry/feature_extractor.py:66: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  return df_f.fillna(0)\n",
      "/home/kian/Desktop/Clean_Projects/ForgeX4-COSMOS-Omega/cosmos/foundry/feature_extractor.py:66: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  return df_f.fillna(0)\n",
      "/home/kian/Desktop/Clean_Projects/ForgeX4-COSMOS-Omega/cosmos/foundry/feature_extractor.py:66: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  return df_f.fillna(0)\n",
      "/home/kian/Desktop/Clean_Projects/ForgeX4-COSMOS-Omega/cosmos/foundry/feature_extractor.py:66: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  return df_f.fillna(0)\n",
      "/home/kian/Desktop/Clean_Projects/ForgeX4-COSMOS-Omega/cosmos/foundry/feature_extractor.py:66: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  return df_f.fillna(0)\n",
      "/home/kian/Desktop/Clean_Projects/ForgeX4-COSMOS-Omega/cosmos/foundry/feature_extractor.py:66: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  return df_f.fillna(0)\n",
      "/home/kian/Desktop/Clean_Projects/ForgeX4-COSMOS-Omega/cosmos/foundry/feature_extractor.py:66: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  return df_f.fillna(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final lexical hybrid training dataframe created. Shape: (6928, 76)\n",
      "Data prepared. X_train shape: (5542, 74)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Generating lexical fingerprint for {TARGET_FILE_PATH.name}...\")\n",
    "with open(TARGET_FILE_PATH, 'r') as f:\n",
    "    cjson_source_code = f.read()\n",
    "static_lexical_features = extract_lexical_features(cjson_source_code)\n",
    "print(\"Lexical features extracted successfully.\")\n",
    "\n",
    "df_list = []\n",
    "print(f\"Loading telemetry data from: {TELEMETRY_DIR}\")\n",
    "for parquet_file in TELEMETRY_DIR.glob('*.parquet'):\n",
    "    df_workload = pd.read_parquet(parquet_file)\n",
    "    df_ts_features = create_time_series_features(df_workload)\n",
    "    \n",
    "    for feature_name, value in static_lexical_features.items():\n",
    "        df_ts_features[feature_name] = value\n",
    "    df_list.append(df_ts_features)\n",
    "\n",
    "df_final = pd.concat(df_list, ignore_index=True).drop(columns=['workload_type'], errors='ignore').fillna(0)\n",
    "print(f\"Final lexical hybrid training dataframe created. Shape: {df_final.shape}\")\n",
    "\n",
    "target = 'cpu_util_core_0'\n",
    "features = [col for col in df_final.columns if col != target and col != 'timestamp']\n",
    "X = df_final[features]\n",
    "y = df_final[target]\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "train_full_indices = X_train_full.index\n",
    "fold_A_indices, fold_B_indices = train_test_split(train_full_indices, test_size=0.5, random_state=42)\n",
    "print(f\"Data prepared. X_train shape: {X_train_full.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e5f6g7",
   "metadata": {},
   "source": [
    "### 4. Training the Definitive v6.0 Lexical Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f6g7h8",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_models = [\n",
    "    LGBMRegressor(random_state=42),\n",
    "    RandomForestRegressor(random_state=42, n_jobs=-1),\n",
    "    ExtraTreesRegressor(random_state=42, n_jobs=-1),\n",
    "    LGBMRegressor(n_estimators=200, random_state=123)\n",
    "]\n",
    "print(\"--- Training Base Models for Stacking ---\")\n",
    "trained_on_A = [copy.deepcopy(m).fit(X_train_full.loc[fold_A_indices], y_train_full.loc[fold_A_indices]) for m in base_models]\n",
    "preds_on_B = np.array([m.predict(X_train_full.loc[fold_B_indices]) for m in trained_on_A]).T\n",
    "trained_on_B = [copy.deepcopy(m).fit(X_train_full.loc[fold_B_indices], y_train_full.loc[fold_B_indices]) for m in base_models]\n",
    "preds_on_A = np.array([m.predict(X_train_full.loc[fold_A_indices]) for m in trained_on_B]).T\n",
    "print(\"Base models trained.\")\n",
    "\n",
    "print(\"\\n--- Creating Augmented Meta-Features ---\")\n",
    "X_meta_train_preds = np.vstack([preds_on_A, preds_on_B])\n",
    "X_meta_train_orig = X_train_full.loc[np.concatenate([fold_A_indices, fold_B_indices])].values\n",
    "X_meta_train = np.hstack([X_meta_train_orig, X_meta_train_preds])\n",
    "y_meta_train = y_train_full.loc[np.concatenate([fold_A_indices, fold_B_indices])]\n",
    "print(f\"Meta-training set shape: {X_meta_train.shape}\")\n",
    "\n",
    "print(\"\\n--- Training Meta-Model ---\")\n",
    "meta_model = RidgeCV()\n",
    "meta_model.fit(X_meta_train, y_meta_train)\n",
    "print(\"Meta-model trained.\")\n",
    "\n",
    "print(\"\\n--- Training Final Base Models for Deployment ---\")\n",
    "final_base_models = [copy.deepcopy(m).fit(X_train_full, y_train_full) for m in base_models]\n",
    "print(\"Final base models trained.\")\n",
    "\n",
    "print(\"\\n--- Evaluating on Test Set ---\")\n",
    "base_test_preds = np.array([m.predict(X_test) for m in final_base_models]).T\n",
    "X_meta_test = np.hstack([X_test.values, base_test_preds])\n",
    "final_predictions = meta_model.predict(X_meta_test)\n",
    "r2 = r2_score(y_test, final_predictions)\n",
    "print(f\"\\n--- Final v6.0 LEXICAL Hybrid Ensemble Performance ---\")\n",
    "print(f\"R-squared (R²): {r2:.4f}\")\n",
    "\n",
    "print(\"\\n--- Saving Artifacts ---\")\n",
    "joblib.dump(final_base_models[0], ENSEMBLE_DIR / 'base_model_A.joblib')\n",
    "joblib.dump(final_base_models[1], ENSEMBLE_DIR / 'base_model_B.joblib')\n",
    "joblib.dump(final_base_models[2], ENSEMBLE_DIR / 'base_model_C.joblib')\n",
    "joblib.dump(final_base_models[3], ENSEMBLE_DIR / 'base_model_D.joblib')\n",
    "joblib.dump(meta_model, ENSEMBLE_DIR / 'meta_model.joblib')\n",
    "print(f\"All 5 model artifacts for the v6.0 Lexical Hybrid Ensemble saved to: {ENSEMBLE_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
