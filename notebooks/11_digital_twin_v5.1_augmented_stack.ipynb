{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6e56c74",
   "metadata": {},
   "source": [
    "# Digital Twin v5.3: The Hybrid Ensemble (Definitive Retraining)\n",
    "\n",
    "**Author:** Kian Mansouri Jamshidi\n",
    "**Project Director:** Kian Mansouri Jamshidi\n",
    "**Date:** 2025-09-27\n",
    "\n",
    "## Objective\n",
    "This notebook creates the definitive v5.3 Hybrid model. It uses the centralized `feature_extractor` utility to guarantee perfect consistency between the training environment and the production `PerformanceTitan`. It combines real telemetry data with real AST features to produce the final, scientifically valid, state-of-the-art model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2850439",
   "metadata": {},
   "source": [
    "### 1. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "501fdb54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artifacts will be saved to: /home/kian/Desktop/ForgeX4-COSMOS-Omega/artifacts/phase2/digital_twin_v5.3_hybrid_ensemble\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "import copy\n",
    "import os\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from pycparser import c_ast\n",
    "\n",
    "# --- Path Setup & Import Single Source of Truth ---\n",
    "PROJECT_ROOT = Path('.').resolve().parent\n",
    "sys.path.append(str(PROJECT_ROOT))\n",
    "from cosmos.parser import parser\n",
    "from cosmos.foundry.feature_extractor import AstFeatureVisitor, create_time_series_features, AST_FEATURES_NAMES\n",
    "\n",
    "TELEMETRY_DIR = PROJECT_ROOT / 'data' / 'telemetry_v2'\n",
    "GENOME_DIR = PROJECT_ROOT / 'data' / 'genomes' / 'cronos'\n",
    "ARTIFACT_DIR = PROJECT_ROOT / 'artifacts' / 'phase2'\n",
    "ENSEMBLE_DIR = ARTIFACT_DIR / 'digital_twin_v5.3_hybrid_ensemble'\n",
    "ENSEMBLE_DIR.mkdir(exist_ok=True)\n",
    "print(f\"Artifacts will be saved to: {ENSEMBLE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b138a92c",
   "metadata": {},
   "source": [
    "### 2. Definitive Hybrid Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9117affd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AST features extracted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kian/Desktop/ForgeX4-COSMOS-Omega/cosmos/foundry/feature_extractor.py:66: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  return df_f.fillna(0)\n",
      "/home/kian/Desktop/ForgeX4-COSMOS-Omega/cosmos/foundry/feature_extractor.py:66: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  return df_f.fillna(0)\n",
      "/home/kian/Desktop/ForgeX4-COSMOS-Omega/cosmos/foundry/feature_extractor.py:66: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  return df_f.fillna(0)\n",
      "/home/kian/Desktop/ForgeX4-COSMOS-Omega/cosmos/foundry/feature_extractor.py:66: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  return df_f.fillna(0)\n",
      "/home/kian/Desktop/ForgeX4-COSMOS-Omega/cosmos/foundry/feature_extractor.py:66: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  return df_f.fillna(0)\n",
      "/home/kian/Desktop/ForgeX4-COSMOS-Omega/cosmos/foundry/feature_extractor.py:66: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  return df_f.fillna(0)\n",
      "/home/kian/Desktop/ForgeX4-COSMOS-Omega/cosmos/foundry/feature_extractor.py:66: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  return df_f.fillna(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final hybrid training dataframe created. Shape: (6928, 69)\n",
      "Data prepared. X_train shape: (5542, 67)\n"
     ]
    }
   ],
   "source": [
    "# 1. Extract REAL AST features using the centralized utility\n",
    "ast_low = parser.parse_c_file_to_ast(str(GENOME_DIR / 'cronos_v1.0.c'))\n",
    "ast_high = parser.parse_c_file_to_ast(str(GENOME_DIR / 'cronos_heavy_compute.c'))\n",
    "visitor = AstFeatureVisitor()\n",
    "features_low_dict = visitor.extract(ast_low)\n",
    "visitor.__init__() # Reset visitor\n",
    "features_high_dict = visitor.extract(ast_high)\n",
    "print(\"AST features extracted.\")\n",
    "\n",
    "# 2. Load REAL telemetry and create the final hybrid dataframe\n",
    "df_list = []\n",
    "for parquet_file in TELEMETRY_DIR.glob('*.parquet'):\n",
    "    df_workload = pd.read_parquet(parquet_file)\n",
    "    # Create time-series features using the centralized utility\n",
    "    df_ts_features = create_time_series_features(df_workload)\n",
    "    \n",
    "    workload_type = df_workload['workload_type'].iloc[0]\n",
    "    ast_features_to_assign = features_high_dict if 'cpu_bound' in workload_type else features_low_dict\n",
    "    for feature_name, value in ast_features_to_assign.items():\n",
    "        df_ts_features[feature_name] = value\n",
    "    df_list.append(df_ts_features)\n",
    "\n",
    "df_final = pd.concat(df_list, ignore_index=True).drop(columns=['workload_type']).fillna(0)\n",
    "print(f\"Final hybrid training dataframe created. Shape: {df_final.shape}\")\n",
    "\n",
    "# 3. Split data for training\n",
    "target = 'cpu_util_core_0'\n",
    "features = [col for col in df_final.columns if col != target and col != 'timestamp']\n",
    "X = df_final[features]\n",
    "y = df_final[target]\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "train_full_indices = X_train_full.index\n",
    "fold_A_indices, fold_B_indices = train_test_split(train_full_indices, test_size=0.5, random_state=42)\n",
    "print(f\"Data prepared. X_train shape: {X_train_full.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff23c7e",
   "metadata": {},
   "source": [
    "### 3. Training the Definitive Hybrid Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1616c5c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training Base Models ---\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.209446 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 2771, number of used features: 58\n",
      "[LightGBM] [Info] Start training from score 4.888488\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000805 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2031\n",
      "[LightGBM] [Info] Number of data points in the train set: 2771, number of used features: 58\n",
      "[LightGBM] [Info] Start training from score 4.888488\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000601 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1943\n",
      "[LightGBM] [Info] Number of data points in the train set: 2771, number of used features: 58\n",
      "[LightGBM] [Info] Start training from score 4.541140\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001154 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1943\n",
      "[LightGBM] [Info] Number of data points in the train set: 2771, number of used features: 58\n",
      "[LightGBM] [Info] Start training from score 4.541140\n",
      "Base models trained.\n",
      "\n",
      "--- Creating Augmented Meta-Features ---\n",
      "Meta-training set shape: (5542, 71)\n",
      "\n",
      "--- Training Meta-Model ---\n",
      "Meta-model trained.\n",
      "\n",
      "--- Training Final Base Models for Deployment ---\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002734 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2373\n",
      "[LightGBM] [Info] Number of data points in the train set: 5542, number of used features: 58\n",
      "[LightGBM] [Info] Start training from score 4.714814\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002484 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2373\n",
      "[LightGBM] [Info] Number of data points in the train set: 5542, number of used features: 58\n",
      "[LightGBM] [Info] Start training from score 4.714814\n",
      "Final base models trained.\n",
      "\n",
      "--- Evaluating on Test Set ---\n",
      "\n",
      "--- Final v5.3 HYBRID Ensemble Performance ---\n",
      "R-squared (R²): 0.8860\n",
      "\n",
      "--- Saving Artifacts ---\n",
      "All 5 model artifacts for the v5.3 Hybrid Ensemble saved to: /home/kian/Desktop/ForgeX4-COSMOS-Omega/artifacts/phase2/digital_twin_v5.3_hybrid_ensemble\n"
     ]
    }
   ],
   "source": [
    "base_models = [\n",
    "    LGBMRegressor(random_state=42),\n",
    "    RandomForestRegressor(random_state=42, n_jobs=-1),\n",
    "    ExtraTreesRegressor(random_state=42, n_jobs=-1),\n",
    "    LGBMRegressor(n_estimators=200, random_state=123)\n",
    "]\n",
    "print(\"--- Training Base Models ---\")\n",
    "trained_on_A = [copy.deepcopy(m).fit(X_train_full.loc[fold_A_indices], y_train_full.loc[fold_A_indices]) for m in base_models]\n",
    "preds_on_B = np.array([m.predict(X_train_full.loc[fold_B_indices]) for m in trained_on_A]).T\n",
    "trained_on_B = [copy.deepcopy(m).fit(X_train_full.loc[fold_B_indices], y_train_full.loc[fold_B_indices]) for m in base_models]\n",
    "preds_on_A = np.array([m.predict(X_train_full.loc[fold_A_indices]) for m in trained_on_B]).T\n",
    "print(\"Base models trained.\")\n",
    "\n",
    "print(\"\\n--- Creating Augmented Meta-Features ---\")\n",
    "X_meta_train_preds = np.vstack([preds_on_A, preds_on_B])\n",
    "X_meta_train_orig = X_train_full.loc[np.concatenate([fold_A_indices, fold_B_indices])].values\n",
    "X_meta_train = np.hstack([X_meta_train_orig, X_meta_train_preds])\n",
    "y_meta_train = y_train_full.loc[np.concatenate([fold_A_indices, fold_B_indices])]\n",
    "print(f\"Meta-training set shape: {X_meta_train.shape}\")\n",
    "\n",
    "print(\"\\n--- Training Meta-Model ---\")\n",
    "meta_model = RidgeCV()\n",
    "meta_model.fit(X_meta_train, y_meta_train)\n",
    "print(\"Meta-model trained.\")\n",
    "\n",
    "print(\"\\n--- Training Final Base Models for Deployment ---\")\n",
    "final_base_models = [copy.deepcopy(m).fit(X_train_full, y_train_full) for m in base_models]\n",
    "print(\"Final base models trained.\")\n",
    "\n",
    "print(\"\\n--- Evaluating on Test Set ---\")\n",
    "base_test_preds = np.array([m.predict(X_test) for m in final_base_models]).T\n",
    "X_meta_test = np.hstack([X_test.values, base_test_preds])\n",
    "final_predictions = meta_model.predict(X_meta_test)\n",
    "r2 = r2_score(y_test, final_predictions)\n",
    "print(f\"\\n--- Final v5.3 HYBRID Ensemble Performance ---\")\n",
    "print(f\"R-squared (R²): {r2:.4f}\")\n",
    "\n",
    "print(\"\\n--- Saving Artifacts ---\")\n",
    "joblib.dump(final_base_models[0], ENSEMBLE_DIR / 'base_model_A.joblib')\n",
    "joblib.dump(final_base_models[1], ENSEMBLE_DIR / 'base_model_B.joblib')\n",
    "joblib.dump(final_base_models[2], ENSEMBLE_DIR / 'base_model_C.joblib')\n",
    "joblib.dump(final_base_models[3], ENSEMBLE_DIR / 'base_model_D.joblib')\n",
    "joblib.dump(meta_model, ENSEMBLE_DIR / 'meta_model.joblib')\n",
    "print(f\"All 5 model artifacts for the v5.3 Hybrid Ensemble saved to: {ENSEMBLE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764c116f-eb36-4460-bbba-216255c14355",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
