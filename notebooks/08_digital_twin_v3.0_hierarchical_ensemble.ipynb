{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b22299e7",
   "metadata": {},
   "source": [
    "# Digital Twin v3.0: Hierarchical Stacking Ensemble\n",
    "\n",
    "**Author:** Kian Mansouri Jamshidi\n",
    "**Project Director:** Kian Mansouri Jamshidi\n",
    "**Date:** 2025-09-27\n",
    "\n",
    "## Objective\n",
    "This is the ultimate experiment of Sprint 5. We will construct a multi-layered, hierarchical stacking ensemble—a true 'network of AIs'—to achieve the maximum possible performance. This architecture synthesizes the predictions of our best models through multiple layers of abstraction, culminating in a single 'Superior AI' prediction. This is our definitive attempt to break the 0.85 R² barrier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12bfa0c",
   "metadata": {},
   "source": [
    "### 1. Imports and Data Preparation\n",
    "\n",
    "We load all necessary libraries and our three diverse champion models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fc2576a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded.\n",
      "All three base models loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.linear_model import RidgeCV\n",
    "import lightgbm as lgb\n",
    "\n",
    "# --- Load Data ---\n",
    "PROJECT_ROOT = Path('.').resolve().parent\n",
    "TELEMETRY_DIR = PROJECT_ROOT / 'data' / 'telemetry_v2'\n",
    "ARTIFACT_DIR = PROJECT_ROOT / 'artifacts' / 'phase2'\n",
    "df_list = [pd.read_parquet(file) for file in glob.glob(str(TELEMETRY_DIR / \"*.parquet\"))]\n",
    "df = pd.concat(df_list, ignore_index=True).sort_values(by='timestamp').reset_index(drop=True)\n",
    "if 'cpu_temp_celsius_avg' in df.columns:\n",
    "    df = df.drop('cpu_temp_celsius_avg', axis=1)\n",
    "print(\"Data loaded.\")\n",
    "\n",
    "# --- Load Pre-trained Base Models (Layer 1) ---\n",
    "model_v2_0 = joblib.load(ARTIFACT_DIR / 'digital_twin_v2.0.joblib')\n",
    "model_v2_1 = joblib.load(ARTIFACT_DIR / 'digital_twin_v2.1_tuned.joblib')\n",
    "model_v2_3 = joblib.load(ARTIFACT_DIR / 'digital_twin_v2.3_deep.joblib')\n",
    "print(\"All three base models loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c293a6",
   "metadata": {},
   "source": [
    "### 2. Full Feature Generation and Alignment\n",
    "\n",
    "We must create the three distinct feature sets for our base models and ensure they are perfectly aligned to the same set of data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5a0e7b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data aligned. Final dataset size: 6899 rows.\n"
     ]
    }
   ],
   "source": [
    "# --- Feature Engineering Functions --- #\n",
    "def create_v2_0_features(df):\n",
    "    # This function creates features for v2.0 and v2.1 models\n",
    "    df_f = df.copy()\n",
    "    workload_dummies = pd.get_dummies(df_f['workload_type'], prefix='workload')\n",
    "    df_f = pd.concat([df_f, workload_dummies], axis=1)\n",
    "    df_f['overall_util_rolling_mean'] = df_f['cpu_util_overall'].rolling(window=10).mean()\n",
    "    df_f['overall_util_rolling_std'] = df_f['cpu_util_overall'].rolling(window=10).std()\n",
    "    other_cores = [c for c in df.columns if 'cpu_util_core' in c and c != 'cpu_util_core_0']\n",
    "    for i in range(1, 6):\n",
    "        df_f[f'overall_util_lag_{i}'] = df_f['cpu_util_overall'].shift(i)\n",
    "        for core in other_cores:\n",
    "            df_f[f'{core}_lag_{i}'] = df_f[core].shift(i)\n",
    "    return df_f.drop('workload_type', axis=1).dropna().reset_index(drop=True)\n",
    "\n",
    "def create_v2_3_features(df):\n",
    "    df_f = df.copy()\n",
    "    workload_dummies = pd.get_dummies(df_f['workload_type'], prefix='workload')\n",
    "    df_f = pd.concat([df_f, workload_dummies], axis=1)\n",
    "    df_f['overall_util_rolling_mean'] = df_f['cpu_util_overall'].rolling(window=30).mean()\n",
    "    df_f['overall_util_rolling_std'] = df_f['cpu_util_overall'].rolling(window=30).std()\n",
    "    other_cores = [c for c in df.columns if 'cpu_util_core' in c and c != 'cpu_util_core_0']\n",
    "    for i in range(1, 21):\n",
    "        df_f[f'overall_util_lag_{i}'] = df_f['cpu_util_overall'].shift(i)\n",
    "        if i <= 10:\n",
    "            for core in other_cores:\n",
    "                df_f[f'{core}_lag_{i}'] = df_f[core].shift(i)\n",
    "    return df_f.drop('workload_type', axis=1).dropna().reset_index(drop=True)\n",
    "\n",
    "# --- Create and Align --- #\n",
    "df_v2_0 = create_v2_0_features(df)\n",
    "df_v2_3 = create_v2_3_features(df)\n",
    "target = 'cpu_util_core_0'\n",
    "\n",
    "common_indices = df_v2_0.index.intersection(df_v2_3.index)\n",
    "df_aligned_v2_0 = df_v2_0.loc[common_indices]\n",
    "df_aligned_v2_3 = df_v2_3.loc[common_indices]\n",
    "\n",
    "features_v2_0_cols = [c for c in df_aligned_v2_0.columns if ('cpu_util' in c and c != target) or 'workload_' in c]\n",
    "features_v2_3_cols = [c for c in df_aligned_v2_3.columns if ('cpu_util' in c and c != target) or 'workload_' in c]\n",
    "\n",
    "X_v2_0 = df_aligned_v2_0[features_v2_0_cols]\n",
    "X_v2_3 = df_aligned_v2_3[features_v2_3_cols]\n",
    "y = df_aligned_v2_0[target]\n",
    "\n",
    "print(f\"Data aligned. Final dataset size: {len(y)} rows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2cc755",
   "metadata": {},
   "source": [
    "### 3. Hierarchical Training\n",
    "\n",
    "This is the core of the process. We will use cross-validation to generate out-of-fold predictions to train each layer of the hierarchy, preventing data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "656f7922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stage 1: Generating Layer 1 Predictions ---\n",
      "--- Stage 2: Training Layer 2 Meta-Models ---\n",
      "--- Stage 3: Training Layer 3 Unifying Model ---\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000081 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 510\n",
      "[LightGBM] [Info] Number of data points in the train set: 5519, number of used features: 2\n",
      "Hierarchical training complete.\n"
     ]
    }
   ],
   "source": [
    "# Split data into a main training set and a final, unseen hold-out test set\n",
    "indices = np.arange(len(y))\n",
    "train_indices, test_indices = train_test_split(indices, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"--- Stage 1: Generating Layer 1 Predictions ---\")\n",
    "# Use cross_val_predict to get out-of-fold predictions for the training set\n",
    "layer1_preds_train_v2_0 = cross_val_predict(model_v2_0, X_v2_0.iloc[train_indices], y.iloc[train_indices], cv=5, n_jobs=-1)\n",
    "layer1_preds_train_v2_1 = cross_val_predict(model_v2_1, X_v2_0.iloc[train_indices], y.iloc[train_indices], cv=5, n_jobs=-1)\n",
    "layer1_preds_train_v2_3 = cross_val_predict(model_v2_3, X_v2_3.iloc[train_indices], y.iloc[train_indices], cv=5, n_jobs=-1)\n",
    "\n",
    "# Create the training set for Layer 2\n",
    "X_meta_train_L2 = pd.DataFrame({\n",
    "    'pred_v2_0': layer1_preds_train_v2_0,\n",
    "    'pred_v2_1': layer1_preds_train_v2_1,\n",
    "    'pred_v2_3': layer1_preds_train_v2_3\n",
    "})\n",
    "y_train_L2 = y.iloc[train_indices]\n",
    "\n",
    "print(\"--- Stage 2: Training Layer 2 Meta-Models ---\")\n",
    "meta_model_X = RidgeCV()\n",
    "meta_model_Y = lgb.LGBMRegressor(random_state=42, n_jobs=-1)\n",
    "\n",
    "# Get out-of-fold predictions for Layer 2 to train Layer 3\n",
    "layer2_preds_train_X = cross_val_predict(meta_model_X, X_meta_train_L2, y_train_L2, cv=5, n_jobs=-1)\n",
    "layer2_preds_train_Y = cross_val_predict(meta_model_Y, X_meta_train_L2, y_train_L2, cv=5, n_jobs=-1)\n",
    "\n",
    "# Create the training set for Layer 3\n",
    "X_meta_train_L3 = pd.DataFrame({\n",
    "    'pred_meta_X': layer2_preds_train_X,\n",
    "    'pred_meta_Y': layer2_preds_train_Y\n",
    "})\n",
    "y_train_L3 = y_train_L2\n",
    "\n",
    "print(\"--- Stage 3: Training Layer 3 Unifying Model ---\")\n",
    "unifying_model = lgb.LGBMRegressor(**model_v2_3.get_params()) # Use the powerful deep history params\n",
    "unifying_model.fit(X_meta_train_L3, y_train_L3)\n",
    "\n",
    "print(\"Hierarchical training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f33e14",
   "metadata": {},
   "source": [
    "### 4. Final Evaluation of the Full Hierarchy\n",
    "\n",
    "Now we will pass our unseen hold-out test set through the entire trained pipeline to get our final prediction and score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d60171cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluating on hold-out test set ---\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000109 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 765\n",
      "[LightGBM] [Info] Number of data points in the train set: 5519, number of used features: 3\n",
      "[LightGBM] [Info] Start training from score 4.857456\n",
      "\n",
      "--- Final Hierarchical Ensemble Performance ---\n",
      "R-squared (R²): 0.7183\n",
      "\n",
      "LIMIT REACHED: The hierarchy did not improve upon the simpler ensemble. The V2.5 result remains the peak.\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Evaluating on hold-out test set ---\")\n",
    "\n",
    "# Layer 1 Predictions on Test Set\n",
    "layer1_preds_test_v2_0 = model_v2_0.predict(X_v2_0.iloc[test_indices])\n",
    "layer1_preds_test_v2_1 = model_v2_1.predict(X_v2_0.iloc[test_indices])\n",
    "layer1_preds_test_v2_3 = model_v2_3.predict(X_v2_3.iloc[test_indices])\n",
    "X_meta_test_L2 = pd.DataFrame({\n",
    "    'pred_v2_0': layer1_preds_test_v2_0,\n",
    "    'pred_v2_1': layer1_preds_test_v2_1,\n",
    "    'pred_v2_3': layer1_preds_test_v2_3\n",
    "})\n",
    "\n",
    "# Layer 2 Predictions on Test Set (Need to train the L2 models first on FULL L1 training data)\n",
    "meta_model_X.fit(X_meta_train_L2, y_train_L2)\n",
    "meta_model_Y.fit(X_meta_train_L2, y_train_L2)\n",
    "layer2_preds_test_X = meta_model_X.predict(X_meta_test_L2)\n",
    "layer2_preds_test_Y = meta_model_Y.predict(X_meta_test_L2)\n",
    "X_meta_test_L3 = pd.DataFrame({\n",
    "    'pred_meta_X': layer2_preds_test_X,\n",
    "    'pred_meta_Y': layer2_preds_test_Y\n",
    "})\n",
    "\n",
    "# Layer 3 (Final) Prediction\n",
    "final_predictions = unifying_model.predict(X_meta_test_L3)\n",
    "\n",
    "# The true values for our test set\n",
    "y_test = y.iloc[test_indices]\n",
    "\n",
    "r2 = r2_score(y_test, final_predictions)\n",
    "\n",
    "print(f\"\\n--- Final Hierarchical Ensemble Performance ---\")\n",
    "print(f\"R-squared (R²): {r2:.4f}\")\n",
    "\n",
    "if r2 >= 0.85:\n",
    "    print(\"\\nMISSION ACCOMPLISHED: The Hierarchical Ensemble has broken the 85% barrier!\")\n",
    "elif r2 > 0.7436:\n",
    "    print(\"\\nULTIMATE BREAKTHROUGH: The hierarchy is superior to all previous models.\")\n",
    "else:\n",
    "    print(\"\\nLIMIT REACHED: The hierarchy did not improve upon the simpler ensemble. The V2.5 result remains the peak.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d9406c-6545-4aee-b749-67f3dfc9a4c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
